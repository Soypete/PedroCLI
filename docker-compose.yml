version: '3.8'

services:
  # Ollama service for LLM inference
  ollama:
    image: ollama/ollama:latest
    container_name: pedrocli-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped
    # Uncomment for GPU support (NVIDIA)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # PedroCLI service
  pedrocli:
    build: .
    container_name: pedrocli
    depends_on:
      - ollama
    volumes:
      # Mount your project directory
      - ./:/workspace
      # Mount config file
      - ./.pedroceli.json:/root/.pedroceli.json:ro
    environment:
      # Ollama URL (internal docker network)
      OLLAMA_URL: http://ollama:11434
    network_mode: "service:ollama"
    command: help

volumes:
  ollama-data:
    driver: local
